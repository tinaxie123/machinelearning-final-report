"""
ä»åº•å±‚å®ç°çš„PCAå’ŒBERTæ¨¡å‹
ä¸ä½¿ç”¨sklearnå’Œtransformersåº“
"""

import numpy as np
import json
import os
from tqdm import tqdm


class CustomPCA:
    """
    ä»åº•å±‚å®ç°çš„PCAç®—æ³•
    ä½¿ç”¨SVDåˆ†è§£æ¥è®¡ç®—ä¸»æˆåˆ†
    """

    def __init__(self, n_components=100, random_state=None):
        self.n_components = n_components
        self.random_state = random_state
        self.mean_ = None
        self.components_ = None
        self.explained_variance_ = None
        self.explained_variance_ratio_ = None
        self.singular_values_ = None

        if random_state is not None:
            np.random.seed(random_state)

    def fit(self, X):
        self.mean_ = np.mean(X, axis=0)
        X_centered = X - self.mean_

        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

        self.components_ = Vt[:self.n_components]
        self.singular_values_ = S[:self.n_components]

        n_samples = X.shape[0]
        self.explained_variance_ = (S[:self.n_components] ** 2) / (n_samples - 1)

        total_variance = np.sum(S ** 2) / (n_samples - 1)
        self.explained_variance_ratio_ = self.explained_variance_ / total_variance

        return self

    def transform(self, X):
        if self.mean_ is None:
            raise ValueError("PCAæœªæ‹Ÿåˆ,è¯·å…ˆè°ƒç”¨fit()")

        X_centered = X - self.mean_

        X_transformed = np.dot(X_centered, self.components_.T)

        return X_transformed

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)

    def inverse_transform(self, X_transformed):
        if self.components_ is None:
            raise ValueError("PCAæœªæ‹Ÿåˆ,è¯·å…ˆè°ƒç”¨fit()")

        X_original = np.dot(X_transformed, self.components_) + self.mean_

        return X_original


class MultiHeadAttention:
    """å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, hidden_size, num_heads):
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_size = hidden_size // num_heads

        self.query_weight = None
        self.query_bias = None
        self.key_weight = None
        self.key_bias = None
        self.value_weight = None
        self.value_bias = None
        self.output_weight = None
        self.output_bias = None

    def forward(self, hidden_states, attention_mask=None):
        batch_size, seq_len, _ = hidden_states.shape

        query = np.dot(hidden_states, self.query_weight.T) + self.query_bias
        key = np.dot(hidden_states, self.key_weight.T) + self.key_bias
        value = np.dot(hidden_states, self.value_weight.T) + self.value_bias

        query = query.reshape(batch_size, seq_len, self.num_heads, self.head_size)
        key = key.reshape(batch_size, seq_len, self.num_heads, self.head_size)
        value = value.reshape(batch_size, seq_len, self.num_heads, self.head_size)

        query = np.transpose(query, (0, 2, 1, 3))
        key = np.transpose(key, (0, 2, 1, 3))
        value = np.transpose(value, (0, 2, 1, 3))

        attention_scores = np.matmul(query, np.transpose(key, (0, 1, 3, 2)))
        attention_scores = attention_scores / np.sqrt(self.head_size)

        if attention_mask is not None:
            attention_mask = attention_mask[:, np.newaxis, np.newaxis, :]
            attention_scores = attention_scores + (1.0 - attention_mask) * -10000.0

        attention_probs = self._softmax(attention_scores)

        context = np.matmul(attention_probs, value)

        context = np.transpose(context, (0, 2, 1, 3))

        context = context.reshape(batch_size, seq_len, self.hidden_size)

        output = np.dot(context, self.output_weight.T) + self.output_bias

        return output

    def _softmax(self, x):
        x_max = np.max(x, axis=-1, keepdims=True)
        exp_x = np.exp(x - x_max)
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)


class FeedForward:
    """å‰é¦ˆç¥ç»ç½‘ç»œ"""

    def __init__(self, hidden_size, intermediate_size):
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size

        self.dense1_weight = None
        self.dense1_bias = None
        self.dense2_weight = None
        self.dense2_bias = None

    def forward(self, hidden_states):
        hidden = np.dot(hidden_states, self.dense1_weight.T) + self.dense1_bias

        hidden = self._gelu(hidden)

        output = np.dot(hidden, self.dense2_weight.T) + self.dense2_bias

        return output

    def _gelu(self, x):
        return 0.5 * x * (1.0 + np.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * x ** 3)))


class TransformerLayer:
    """Transformerç¼–ç å™¨å±‚"""

    def __init__(self, hidden_size, num_heads, intermediate_size):
        self.attention = MultiHeadAttention(hidden_size, num_heads)
        self.feed_forward = FeedForward(hidden_size, intermediate_size)

        self.ln1_weight = None
        self.ln1_bias = None
        self.ln2_weight = None
        self.ln2_bias = None

        self.eps = 1e-12

    def forward(self, hidden_states, attention_mask=None):
        attention_output = self.attention.forward(hidden_states, attention_mask)
        hidden_states = self._layer_norm(hidden_states + attention_output,
                                         self.ln1_weight, self.ln1_bias)

        ff_output = self.feed_forward.forward(hidden_states)
        output = self._layer_norm(hidden_states + ff_output,
                                  self.ln2_weight, self.ln2_bias)

        return output

    def _layer_norm(self, x, weight, bias):
        mean = np.mean(x, axis=-1, keepdims=True)
        var = np.var(x, axis=-1, keepdims=True)
        normalized = (x - mean) / np.sqrt(var + self.eps)
        return normalized * weight + bias


class CustomBERT:
    """
    ä»åº•å±‚å®ç°çš„BERTæ¨¡å‹
    å®ç°äº†å®Œæ•´çš„Transformerç¼–ç å™¨æ¶æ„
    """

    def __init__(self, model_path='bert-base-chinese', max_length=128):
        self.model_path = model_path
        self.max_length = max_length

        self.vocab_size = 21128
        self.hidden_size = 768
        self.num_hidden_layers = 12
        self.num_attention_heads = 12
        self.intermediate_size = 3072
        self.max_position_embeddings = 512
        self.type_vocab_size = 2

        self.pad_token_id = 0
        self.cls_token_id = 101
        self.sep_token_id = 102

        self.token_embeddings = None
        self.position_embeddings = None
        self.token_type_embeddings = None
        self.embedding_ln_weight = None
        self.embedding_ln_bias = None

        self.layers = [TransformerLayer(self.hidden_size,
                                       self.num_attention_heads,
                                       self.intermediate_size)
                      for _ in range(self.num_hidden_layers)]

        self.pooler_weight = None
        self.pooler_bias = None

        self.vocab = None
        self.inv_vocab = None

        self.eps = 1e-12

        print(f"[åˆå§‹åŒ–] CustomBERTæ¨¡å‹")
        print(f"  hidden_size: {self.hidden_size}")
        print(f"  num_layers: {self.num_hidden_layers}")
        print(f"  num_heads: {self.num_attention_heads}")

    def load_weights(self, weights_dict):
        print("[åŠ è½½] æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡...")

        self.token_embeddings = weights_dict['embeddings.word_embeddings.weight']
        self.position_embeddings = weights_dict['embeddings.position_embeddings.weight']
        self.token_type_embeddings = weights_dict['embeddings.token_type_embeddings.weight']
        self.embedding_ln_weight = weights_dict['embeddings.LayerNorm.weight']
        self.embedding_ln_bias = weights_dict['embeddings.LayerNorm.bias']

        for i in range(self.num_hidden_layers):
            layer = self.layers[i]
            prefix = f'encoder.layer.{i}'

            layer.attention.query_weight = weights_dict[f'{prefix}.attention.self.query.weight']
            layer.attention.query_bias = weights_dict[f'{prefix}.attention.self.query.bias']
            layer.attention.key_weight = weights_dict[f'{prefix}.attention.self.key.weight']
            layer.attention.key_bias = weights_dict[f'{prefix}.attention.self.key.bias']
            layer.attention.value_weight = weights_dict[f'{prefix}.attention.self.value.weight']
            layer.attention.value_bias = weights_dict[f'{prefix}.attention.self.value.bias']
            layer.attention.output_weight = weights_dict[f'{prefix}.attention.output.dense.weight']
            layer.attention.output_bias = weights_dict[f'{prefix}.attention.output.dense.bias']

            layer.ln1_weight = weights_dict[f'{prefix}.attention.output.LayerNorm.weight']
            layer.ln1_bias = weights_dict[f'{prefix}.attention.output.LayerNorm.bias']

            layer.feed_forward.dense1_weight = weights_dict[f'{prefix}.intermediate.dense.weight']
            layer.feed_forward.dense1_bias = weights_dict[f'{prefix}.intermediate.dense.bias']
            layer.feed_forward.dense2_weight = weights_dict[f'{prefix}.output.dense.weight']
            layer.feed_forward.dense2_bias = weights_dict[f'{prefix}.output.dense.bias']

            layer.ln2_weight = weights_dict[f'{prefix}.output.LayerNorm.weight']
            layer.ln2_bias = weights_dict[f'{prefix}.output.LayerNorm.bias']

        self.pooler_weight = weights_dict['pooler.dense.weight']
        self.pooler_bias = weights_dict['pooler.dense.bias']

        print("[OK] æƒé‡åŠ è½½å®Œæˆ")

    def load_vocab(self, vocab_path):
        self.vocab = {}
        self.inv_vocab = {}

        with open(vocab_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                token = line.strip()
                self.vocab[token] = i
                self.inv_vocab[i] = token

        print(f"[OK] è¯æ±‡è¡¨åŠ è½½å®Œæˆ: {len(self.vocab)} tokens")

    def tokenize(self, text):
        tokens = ['[CLS]'] + list(text) + ['[SEP]']

        token_ids = []
        for token in tokens:
            if token in self.vocab:
                token_ids.append(self.vocab[token])
            else:
                token_ids.append(self.vocab.get('[UNK]', 100))

        if len(token_ids) > self.max_length:
            token_ids = token_ids[:self.max_length]

        return token_ids

    def encode_batch(self, texts):
        batch_token_ids = []
        batch_attention_mask = []

        for text in texts:
            token_ids = self.tokenize(text)

            attention_mask = [1] * len(token_ids)

            padding_length = self.max_length - len(token_ids)
            if padding_length > 0:
                token_ids += [self.pad_token_id] * padding_length
                attention_mask += [0] * padding_length

            batch_token_ids.append(token_ids)
            batch_attention_mask.append(attention_mask)

        return np.array(batch_token_ids), np.array(batch_attention_mask)

    def forward(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.shape

        token_embeds = self.token_embeddings[input_ids]

        position_ids = np.arange(seq_len)[np.newaxis, :]
        position_embeds = self.position_embeddings[position_ids]

        token_type_ids = np.zeros((batch_size, seq_len), dtype=np.int32)
        token_type_embeds = self.token_type_embeddings[token_type_ids]

        embeddings = token_embeds + position_embeds + token_type_embeds

        embeddings = self._layer_norm(embeddings, self.embedding_ln_weight, self.embedding_ln_bias)

        hidden_states = embeddings
        for layer in self.layers:
            hidden_states = layer.forward(hidden_states, attention_mask)

        first_token_tensor = hidden_states[:, 0]
        pooled_output = np.dot(first_token_tensor, self.pooler_weight.T) + self.pooler_bias
        pooled_output = np.tanh(pooled_output)

        return pooled_output, hidden_states

    def _layer_norm(self, x, weight, bias):
        mean = np.mean(x, axis=-1, keepdims=True)
        var = np.var(x, axis=-1, keepdims=True)
        normalized = (x - mean) / np.sqrt(var + self.eps)
        return normalized * weight + bias

    def extract_features(self, texts, batch_size=32):
        all_features = []

        if isinstance(texts[0], list):
            texts = [' '.join(tokens) for tokens in texts]

        for i in tqdm(range(0, len(texts), batch_size), desc="  æå–BERTç‰¹å¾"):
            batch_texts = texts[i:i + batch_size]

            input_ids, attention_mask = self.encode_batch(batch_texts)

            pooled_output, _ = self.forward(input_ids, attention_mask)

            all_features.append(pooled_output)

        features = np.vstack(all_features)
        return features


class CustomBERTExtractor:
    """
    ä½¿ç”¨CustomBERTçš„ç‰¹å¾æå–å™¨
    æä¾›ä¸åŸBERTExtractorç›¸åŒçš„æ¥å£
    """

    def __init__(self, model_name='bert-base-chinese', max_length=128, batch_size=32,
                 use_pretrained=True):
        self.model_name = model_name
        self.max_length = max_length
        self.batch_size = batch_size
        self.use_pretrained = use_pretrained

        print(f"[åˆå§‹åŒ–] è‡ªå®šä¹‰BERTç‰¹å¾æå–å™¨")
        print(f"  model: {model_name}")
        print(f"  max_length: {max_length}")
        print(f"  batch_size: {batch_size}")

        self.model = CustomBERT(model_name, max_length)

        if use_pretrained:
            self._load_pretrained_weights()
        else:
            print("[è­¦å‘Š] æœªåŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
            self._initialize_random_weights()

    def _load_pretrained_weights(self):
        try:
            from transformers import BertModel
            import torch

            print(f"[åŠ è½½] ä»transformersåŠ è½½é¢„è®­ç»ƒæƒé‡: {self.model_name}")
            bert_model = BertModel.from_pretrained(self.model_name)

            weights_dict = {}
            for name, param in bert_model.named_parameters():
                weights_dict[name] = param.detach().cpu().numpy()

            self.model.load_weights(weights_dict)

            from transformers import BertTokenizer
            tokenizer = BertTokenizer.from_pretrained(self.model_name)

            vocab_dict = tokenizer.vocab
            self.model.vocab = vocab_dict
            self.model.inv_vocab = {v: k for k, v in vocab_dict.items()}

            print("[OK] é¢„è®­ç»ƒæƒé‡åŠ è½½å®Œæˆ")

        except Exception as e:
            print(f"[è­¦å‘Š] æ— æ³•åŠ è½½é¢„è®­ç»ƒæƒé‡: {e}")
            print("[è­¦å‘Š] å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
            self._initialize_random_weights()

    def _initialize_random_weights(self):
        self.model.token_embeddings = np.random.randn(self.model.vocab_size,
                                                      self.model.hidden_size) * 0.02
        self.model.position_embeddings = np.random.randn(self.model.max_position_embeddings,
                                                         self.model.hidden_size) * 0.02
        self.model.token_type_embeddings = np.random.randn(self.model.type_vocab_size,
                                                           self.model.hidden_size) * 0.02
        self.model.embedding_ln_weight = np.ones(self.model.hidden_size)
        self.model.embedding_ln_bias = np.zeros(self.model.hidden_size)

        for layer in self.model.layers:
            layer.attention.query_weight = np.random.randn(self.model.hidden_size,
                                                          self.model.hidden_size) * 0.02
            layer.attention.query_bias = np.zeros(self.model.hidden_size)
            layer.attention.key_weight = np.random.randn(self.model.hidden_size,
                                                        self.model.hidden_size) * 0.02
            layer.attention.key_bias = np.zeros(self.model.hidden_size)
            layer.attention.value_weight = np.random.randn(self.model.hidden_size,
                                                          self.model.hidden_size) * 0.02
            layer.attention.value_bias = np.zeros(self.model.hidden_size)
            layer.attention.output_weight = np.random.randn(self.model.hidden_size,
                                                           self.model.hidden_size) * 0.02
            layer.attention.output_bias = np.zeros(self.model.hidden_size)

            layer.ln1_weight = np.ones(self.model.hidden_size)
            layer.ln1_bias = np.zeros(self.model.hidden_size)
            layer.ln2_weight = np.ones(self.model.hidden_size)
            layer.ln2_bias = np.zeros(self.model.hidden_size)

            layer.feed_forward.dense1_weight = np.random.randn(self.model.intermediate_size,
                                                              self.model.hidden_size) * 0.02
            layer.feed_forward.dense1_bias = np.zeros(self.model.intermediate_size)
            layer.feed_forward.dense2_weight = np.random.randn(self.model.hidden_size,
                                                              self.model.intermediate_size) * 0.02
            layer.feed_forward.dense2_bias = np.zeros(self.model.hidden_size)

        self.model.pooler_weight = np.random.randn(self.model.hidden_size,
                                                   self.model.hidden_size) * 0.02
        self.model.pooler_bias = np.zeros(self.model.hidden_size)

        self.model.vocab = {chr(i): i for i in range(256)}
        self.model.vocab['[PAD]'] = 0
        self.model.vocab['[CLS]'] = 101
        self.model.vocab['[SEP]'] = 102
        self.model.vocab['[UNK]'] = 100
        self.model.inv_vocab = {v: k for k, v in self.model.vocab.items()}

    def transform(self, X_tokens):
        return self.model.extract_features(X_tokens, self.batch_size)


def load_thucnews_data(preprocess_path='features/preprocessed/preprocessed_data.pkl'):
    import pickle
    import os

    if not os.path.exists(preprocess_path):
        print(f"[è­¦å‘Š] é¢„å¤„ç†æ•°æ®ä¸å­˜åœ¨: {preprocess_path}")
        return None, None

    with open(preprocess_path, 'rb') as f:
        data = pickle.load(f)

    return data['X_train_tokens'], data['y_train']


def compare_pca_implementations(X, n_components=100, n_samples_display=5):
    print("\n" + "=" * 80)
    print("PCAå®ç°å¯¹æ¯”: CustomPCA vs sklearn.PCA")
    print("=" * 80)

    print(f"\nè¾“å…¥æ•°æ®:")
    print(f"  å½¢çŠ¶: {X.shape}")
    print(f"  å‡å€¼: {X.mean():.6f}")
    print(f"  æ ‡å‡†å·®: {X.std():.6f}")

    print(f"\n[1] ä½¿ç”¨CustomPCAé™ç»´ ({X.shape[1]} -> {n_components})...")
    import time
    start_time = time.time()
    custom_pca = CustomPCA(n_components=n_components, random_state=42)
    X_custom = custom_pca.fit_transform(X)
    custom_time = time.time() - start_time

    print(f"  é™ç»´åå½¢çŠ¶: {X_custom.shape}")
    print(f"  ç´¯è®¡è§£é‡Šæ–¹å·®: {np.sum(custom_pca.explained_variance_ratio_):.6f}")
    print(f"  è€—æ—¶: {custom_time:.4f}ç§’")

    try:
        from sklearn.decomposition import PCA as SklearnPCA

        print(f"\n[2] ä½¿ç”¨sklearn.PCAé™ç»´...")
        start_time = time.time()
        sklearn_pca = SklearnPCA(n_components=n_components, random_state=42)
        X_sklearn = sklearn_pca.fit_transform(X)
        sklearn_time = time.time() - start_time

        print(f"  é™ç»´åå½¢çŠ¶: {X_sklearn.shape}")
        print(f"  ç´¯è®¡è§£é‡Šæ–¹å·®: {np.sum(sklearn_pca.explained_variance_ratio_):.6f}")
        print(f"  è€—æ—¶: {sklearn_time:.4f}ç§’")

        print("\n[3] è¯¦ç»†å¯¹æ¯”")
        print("-" * 80)

        print(f"\nå‰{n_samples_display}ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”ä¾‹å¯¹æ¯”:")
        print(f"{'':>5} {'CustomPCA':>12} {'sklearn':>12} {'å·®å¼‚':>12}")
        print("-" * 45)
        for i in range(min(n_samples_display, n_components)):
            custom_var = custom_pca.explained_variance_ratio_[i]
            sklearn_var = sklearn_pca.explained_variance_ratio_[i]
            diff = abs(custom_var - sklearn_var)
            print(f"PC{i+1:>2}  {custom_var:>12.8f} {sklearn_var:>12.8f} {diff:>12.2e}")

        var_ratio_diff = np.abs(custom_pca.explained_variance_ratio_ -
                                sklearn_pca.explained_variance_ratio_)
        print(f"\nè§£é‡Šæ–¹å·®æ¯”ä¾‹å·®å¼‚ç»Ÿè®¡:")
        print(f"  æœ€å¤§å·®å¼‚: {np.max(var_ratio_diff):.2e}")
        print(f"  å¹³å‡å·®å¼‚: {np.mean(var_ratio_diff):.2e}")
        print(f"  ä¸­ä½æ•°å·®å¼‚: {np.median(var_ratio_diff):.2e}")

        X_diff = np.abs(X_custom - X_sklearn)
        print(f"\nè½¬æ¢åæ•°æ®å·®å¼‚ç»Ÿè®¡:")
        print(f"  æœ€å¤§å·®å¼‚: {np.max(X_diff):.6f}")
        print(f"  å¹³å‡å·®å¼‚: {np.mean(X_diff):.6f}")
        print(f"  ä¸­ä½æ•°å·®å¼‚: {np.median(X_diff):.6f}")

        X_custom_reconstructed = custom_pca.inverse_transform(X_custom)
        X_sklearn_reconstructed = sklearn_pca.inverse_transform(X_sklearn)

        custom_mse = np.mean((X - X_custom_reconstructed) ** 2)
        sklearn_mse = np.mean((X - X_sklearn_reconstructed) ** 2)

        print(f"\né‡æ„è¯¯å·®(MSE):")
        print(f"  CustomPCA: {custom_mse:.10f}")
        print(f"  sklearn:   {sklearn_mse:.10f}")
        print(f"  å·®å¼‚:      {abs(custom_mse - sklearn_mse):.2e}")

        print(f"\næ€§èƒ½å¯¹æ¯”:")
        print(f"  CustomPCA: {custom_time:.4f}ç§’")
        print(f"  sklearn:   {sklearn_time:.4f}ç§’")
        print(f"  é€Ÿåº¦æ¯”:    {custom_time/sklearn_time:.2f}x")

        print("\n" + "=" * 80)
        if np.max(var_ratio_diff) < 1e-8:
            print("âœ… éªŒè¯é€šè¿‡: CustomPCAä¸sklearn.PCAç»“æœå®Œå…¨ä¸€è‡´!")
        elif np.max(var_ratio_diff) < 1e-6:
            print("âœ… éªŒè¯é€šè¿‡: CustomPCAä¸sklearn.PCAç»“æœåŸºæœ¬ä¸€è‡´(è¯¯å·®å¯æ¥å—)")
        else:
            print("âš ï¸  è­¦å‘Š: CustomPCAä¸sklearn.PCAå­˜åœ¨è¾ƒå¤§å·®å¼‚")
        print("=" * 80)

        return custom_pca, sklearn_pca

    except ImportError:
        print("\n[è­¦å‘Š] sklearnæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
        return custom_pca, None


def compare_bert_implementations(X_tokens, n_samples=10):
    print("\n" + "=" * 80)
    print("BERTå®ç°å¯¹æ¯”: CustomBERT vs transformers.BertModel")
    print("=" * 80)

    X_tokens_sample = X_tokens[:n_samples]

    print(f"\næµ‹è¯•æ•°æ®:")
    print(f"  æ ·æœ¬æ•°: {len(X_tokens_sample)}")
    print(f"  ç¤ºä¾‹[0]: {' '.join(X_tokens_sample[0][:20])}...")

    print(f"\n[1] ä½¿ç”¨CustomBERTæå–ç‰¹å¾...")
    try:
        import time
        start_time = time.time()

        custom_bert = CustomBERTExtractor(
            model_name='bert-base-chinese',
            max_length=128,
            batch_size=min(2, n_samples),
            use_pretrained=True
        )
        X_custom = custom_bert.transform(X_tokens_sample)
        custom_time = time.time() - start_time

        print(f"  ç‰¹å¾å½¢çŠ¶: {X_custom.shape}")
        print(f"  ç‰¹å¾å‡å€¼: {X_custom.mean():.6f}")
        print(f"  ç‰¹å¾æ ‡å‡†å·®: {X_custom.std():.6f}")
        print(f"  è€—æ—¶: {custom_time:.4f}ç§’")

    except Exception as e:
        print(f"  âŒ CustomBERTæå–å¤±è´¥: {e}")
        custom_bert = None
        X_custom = None

    try:
        from transformers import BertTokenizer, BertModel
        import torch

        print(f"\n[2] ä½¿ç”¨transformers.BertModelæå–ç‰¹å¾...")
        start_time = time.time()

        tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        model = BertModel.from_pretrained('bert-base-chinese')
        model.eval()

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)

        X_transformers = []
        batch_texts = [' '.join(tokens) for tokens in X_tokens_sample]

        for text in batch_texts:
            encoded = tokenizer(text, padding=True, truncation=True,
                              max_length=128, return_tensors='pt')
            encoded = {k: v.to(device) for k, v in encoded.items()}

            with torch.no_grad():
                outputs = model(**encoded)

            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            X_transformers.append(cls_embedding[0])

        X_transformers = np.array(X_transformers)
        transformers_time = time.time() - start_time

        print(f"  ç‰¹å¾å½¢çŠ¶: {X_transformers.shape}")
        print(f"  ç‰¹å¾å‡å€¼: {X_transformers.mean():.6f}")
        print(f"  ç‰¹å¾æ ‡å‡†å·®: {X_transformers.std():.6f}")
        print(f"  è€—æ—¶: {transformers_time:.4f}ç§’")

        if X_custom is not None:
            print("\n[3] è¯¦ç»†å¯¹æ¯”")
            print("-" * 80)

            X_diff = np.abs(X_custom - X_transformers)
            print(f"\nç‰¹å¾å·®å¼‚ç»Ÿè®¡:")
            print(f"  æœ€å¤§å·®å¼‚: {np.max(X_diff):.6f}")
            print(f"  å¹³å‡å·®å¼‚: {np.mean(X_diff):.6f}")
            print(f"  ä¸­ä½æ•°å·®å¼‚: {np.median(X_diff):.6f}")

            from numpy.linalg import norm
            cosine_sims = []
            for i in range(len(X_custom)):
                cos_sim = np.dot(X_custom[i], X_transformers[i]) / \
                         (norm(X_custom[i]) * norm(X_transformers[i]))
                cosine_sims.append(cos_sim)

            print(f"\nä½™å¼¦ç›¸ä¼¼åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡ç›¸ä¼¼åº¦: {np.mean(cosine_sims):.6f}")
            print(f"  æœ€å°ç›¸ä¼¼åº¦: {np.min(cosine_sims):.6f}")
            print(f"  æœ€å¤§ç›¸ä¼¼åº¦: {np.max(cosine_sims):.6f}")

            print(f"\næ€§èƒ½å¯¹æ¯”:")
            print(f"  CustomBERT:    {custom_time:.4f}ç§’")
            print(f"  transformers:  {transformers_time:.4f}ç§’")
            print(f"  é€Ÿåº¦æ¯”:        {custom_time/transformers_time:.2f}x")

            print("\n" + "=" * 80)
            if np.mean(cosine_sims) > 0.99:
                print("âœ… éªŒè¯é€šè¿‡: CustomBERTä¸transformers.BertModelç»“æœé«˜åº¦ä¸€è‡´!")
            elif np.mean(cosine_sims) > 0.95:
                print("âœ… éªŒè¯é€šè¿‡: CustomBERTä¸transformers.BertModelç»“æœåŸºæœ¬ä¸€è‡´")
            else:
                print("âš ï¸  è­¦å‘Š: CustomBERTä¸transformers.BertModelå­˜åœ¨å·®å¼‚")
            print("=" * 80)

    except ImportError:
        print("\n[è­¦å‘Š] transformersæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
    except Exception as e:
        print(f"\n[é”™è¯¯] transformerså¯¹æ¯”å¤±è´¥: {e}")


if __name__ == '__main__':
    """
    å¯åœ¨Jupyter Notebookä¸­è¿è¡Œçš„æµ‹è¯•ä»£ç 
    å°†æ¯ä¸ª"# ===== å•å…ƒæ ¼ N ====="éƒ¨åˆ†å¤åˆ¶åˆ°æ–°çš„notebookå•å…ƒæ ¼ä¸­
    """

    import pickle

    # ===== å•å…ƒæ ¼ 1: åˆå§‹åŒ– =====
    print("=" * 80)
    print("è‡ªå®šä¹‰PCAå’ŒBERTå®ç° - THUCNewsæ•°æ®é›†æµ‹è¯•")
    print("=" * 80)
    print("\næœ¬æµ‹è¯•ä½¿ç”¨å·²é¢„å¤„ç†çš„THUCNewsæ•°æ®å’Œå·²æå–çš„BERTç‰¹å¾")

    # ===== å•å…ƒæ ¼ 2: æµ‹è¯•CustomPCAåŸºæœ¬åŠŸèƒ½ =====
    print("\n" + "=" * 80)
    print("[æµ‹è¯•1] CustomPCAåŸºæœ¬åŠŸèƒ½")
    print("=" * 80)

    np.random.seed(42)
    X_test = np.random.randn(100, 768)

    print(f"\næµ‹è¯•æ•°æ®å½¢çŠ¶: {X_test.shape}")

    pca = CustomPCA(n_components=100, random_state=42)
    X_pca = pca.fit_transform(X_test)

    print(f"é™ç»´åå½¢çŠ¶: {X_pca.shape}")
    print(f"\nå‰5ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®:")
    for i in range(5):
        print(f"  PC{i+1}: {pca.explained_variance_ratio_[i]:.6f}")
    print(f"\nç´¯è®¡è§£é‡Šæ–¹å·®: {np.sum(pca.explained_variance_ratio_):.6f}")

    X_reconstructed = pca.inverse_transform(X_pca)
    reconstruction_error = np.mean((X_test - X_reconstructed) ** 2)
    print(f"é‡æ„è¯¯å·®(MSE): {reconstruction_error:.10f}")
    print("\nâœ… CustomPCAåŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡!")

    # ===== å•å…ƒæ ¼ 3: ä¸sklearnå¯¹æ¯” =====
    print("\n" + "=" * 80)
    print("[æµ‹è¯•2] CustomPCA vs sklearn.PCA")
    print("=" * 80)

    compare_pca_implementations(X_test, n_components=100, n_samples_display=10)

    # ===== å•å…ƒæ ¼ 4: åŠ è½½THUCNewsé¢„å¤„ç†æ•°æ® =====
    print("\n" + "=" * 80)
    print("[æµ‹è¯•3] åŠ è½½THUCNewsé¢„å¤„ç†æ•°æ®")
    print("=" * 80)

    preprocess_path = 'features/preprocessed/preprocessed_data.pkl'

    if os.path.exists(preprocess_path):
        with open(preprocess_path, 'rb') as f:
            data = pickle.load(f)

        X_train_tokens = data['X_train_tokens']
        X_val_tokens = data['X_val_tokens']
        X_test_tokens = data['X_test_tokens']
        y_train = data['y_train']
        y_val = data['y_val']
        y_test = data['y_test']

        print(f"âœ… æˆåŠŸåŠ è½½é¢„å¤„ç†æ•°æ®")
        print(f"  è®­ç»ƒé›†: {len(X_train_tokens)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(X_val_tokens)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(X_test_tokens)} æ ·æœ¬")
        print(f"  ç±»åˆ«æ•°: {len(np.unique(y_train))}")

        print(f"\nç¤ºä¾‹æ•°æ®:")
        print(f"  æ ·æœ¬[0]: {' '.join(X_train_tokens[0][:15])}...")
        print(f"  æ ‡ç­¾: {y_train[0]}")
    else:
        print(f"âŒ é¢„å¤„ç†æ•°æ®æœªæ‰¾åˆ°: {preprocess_path}")
        print("   è¯·å…ˆè¿è¡Œé¢„å¤„ç†è„šæœ¬")
        X_train_tokens = None

    # ===== å•å…ƒæ ¼ 5: åŠ è½½å·²æå–çš„BERTç‰¹å¾ =====
    if X_train_tokens is not None:
        print("\n" + "=" * 80)
        print("[æµ‹è¯•4] åŠ è½½å·²æå–çš„BERTç‰¹å¾")
        print("=" * 80)

        bert_features_path = 'features/bert/bert_features_768d.pkl'

        if os.path.exists(bert_features_path):
            with open(bert_features_path, 'rb') as f:
                bert_data = pickle.load(f)

            X_train_bert = bert_data['X_train']
            X_val_bert = bert_data['X_val']
            X_test_bert = bert_data['X_test']

            print(f"âœ… æˆåŠŸåŠ è½½BERTç‰¹å¾")
            print(f"  è®­ç»ƒé›†: {X_train_bert.shape}")
            print(f"  éªŒè¯é›†: {X_val_bert.shape}")
            print(f"  æµ‹è¯•é›†: {X_test_bert.shape}")
            print(f"\nç‰¹å¾ç»Ÿè®¡:")
            print(f"  å‡å€¼: {X_train_bert.mean():.6f}")
            print(f"  æ ‡å‡†å·®: {X_train_bert.std():.6f}")
            print(f"  èŒƒå›´: [{X_train_bert.min():.6f}, {X_train_bert.max():.6f}]")
        else:
            print(f"âš ï¸  BERTç‰¹å¾æœªæ‰¾åˆ°: {bert_features_path}")
            print("   å°†æå–å°æ ·æœ¬è¿›è¡Œæµ‹è¯•...")
            X_train_bert = None

    # ===== å•å…ƒæ ¼ 6: å¦‚æœæ²¡æœ‰BERTç‰¹å¾ï¼Œåˆ™æå–å°æ ·æœ¬ =====
    if X_train_tokens is not None and X_train_bert is None:
        print("\n" + "=" * 80)
        print("[æµ‹è¯•5] ä½¿ç”¨CustomBERTæå–ç‰¹å¾ï¼ˆå°æ ·æœ¬ï¼‰")
        print("=" * 80)

        n_test_samples = 20
        print(f"ä½¿ç”¨å‰{n_test_samples}ä¸ªæ ·æœ¬æµ‹è¯•...")

        bert_extractor = CustomBERTExtractor(
            model_name='bert-base-chinese',
            max_length=128,
            batch_size=4,
            use_pretrained=False  # ä½¿ç”¨éšæœºæƒé‡å¿«é€Ÿæµ‹è¯•
        )

        X_train_bert = bert_extractor.transform(X_train_tokens[:n_test_samples])

        print(f"\nBERTç‰¹å¾å½¢çŠ¶: {X_train_bert.shape}")
        print(f"ç‰¹å¾å‡å€¼: {X_train_bert.mean():.6f}")
        print(f"ç‰¹å¾æ ‡å‡†å·®: {X_train_bert.std():.6f}")
        print("\nâœ… CustomBERTç‰¹å¾æå–æµ‹è¯•é€šè¿‡!")

    # ===== å•å…ƒæ ¼ 7: åœ¨BERTç‰¹å¾ä¸Šåº”ç”¨CustomPCA =====
    if X_train_tokens is not None and X_train_bert is not None:
        print("\n" + "=" * 80)
        print("[æµ‹è¯•6] CustomPCAé™ç»´BERTç‰¹å¾")
        print("=" * 80)

        # ç¡®å®šä¸»æˆåˆ†æ•°é‡
        n_components = min(100, X_train_bert.shape[0], X_train_bert.shape[1])
        print(f"ç›®æ ‡ä¸»æˆåˆ†æ•°: {n_components}")

        # åº”ç”¨PCA
        print("\nåº”ç”¨CustomPCAé™ç»´...")
        pca_bert = CustomPCA(n_components=n_components, random_state=42)
        X_train_pca = pca_bert.fit_transform(X_train_bert)

        print(f"\nåŸå§‹BERTç‰¹å¾: {X_train_bert.shape}")
        print(f"PCAé™ç»´å: {X_train_pca.shape}")

        # æ˜¾ç¤ºæ–¹å·®è§£é‡Š
        print(f"\nç´¯è®¡è§£é‡Šæ–¹å·®: {np.sum(pca_bert.explained_variance_ratio_):.6f}")
        print(f"\nå‰10ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®:")
        for i in range(min(10, n_components)):
            print(f"  PC{i+1}: {pca_bert.explained_variance_ratio_[i]:.6f}")

        # æµ‹è¯•é‡æ„
        X_reconstructed = pca_bert.inverse_transform(X_train_pca)
        reconstruction_error = np.mean((X_train_bert - X_reconstructed) ** 2)
        print(f"\né‡æ„è¯¯å·®(MSE): {reconstruction_error:.10f}")

        print("\nâœ… CustomPCAé™ç»´BERTç‰¹å¾æµ‹è¯•é€šè¿‡!")

    # ===== å•å…ƒæ ¼ 8: ä¸å·²ä¿å­˜çš„PCAç‰¹å¾å¯¹æ¯” =====
    if X_train_tokens is not None and X_train_bert is not None:
        print("\n" + "=" * 80)
        print("[æµ‹è¯•7] ä¸å·²ä¿å­˜çš„PCAç‰¹å¾å¯¹æ¯”")
        print("=" * 80)

        bert_pca_path = 'features/bert/bert_features_pca_100d.pkl'

        if os.path.exists(bert_pca_path):
            with open(bert_pca_path, 'rb') as f:
                saved_pca_data = pickle.load(f)

            X_train_saved_pca = saved_pca_data['X_train']

            print(f"å·²ä¿å­˜çš„PCAç‰¹å¾: {X_train_saved_pca.shape}")
            print(f"å½“å‰æå–çš„PCAç‰¹å¾: {X_train_pca.shape}")

            # å¦‚æœç»´åº¦åŒ¹é…ï¼Œæ¯”è¾ƒç‰¹å¾
            if X_train_saved_pca.shape == X_train_pca.shape:
                # PCAçš„ç¬¦å·å¯èƒ½ç›¸åï¼Œæ‰€ä»¥å–ç»å¯¹å€¼æ¯”è¾ƒ
                correlation = np.abs(np.corrcoef(
                    X_train_saved_pca[:, 0],
                    X_train_pca[:, 0]
                )[0, 1])
                print(f"\nç¬¬ä¸€ä¸»æˆåˆ†ç›¸å…³æ€§: {correlation:.6f}")

                if correlation > 0.99:
                    print("âœ… ä¸å·²ä¿å­˜çš„PCAç‰¹å¾é«˜åº¦ä¸€è‡´!")
                else:
                    print("âš ï¸  ä¸å·²ä¿å­˜çš„PCAç‰¹å¾æœ‰å·®å¼‚")
                    print("   (å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„BERTç‰¹å¾æˆ–éšæœºç§å­)")
            else:
                print("âš ï¸  ç‰¹å¾ç»´åº¦ä¸åŒï¼Œæ— æ³•ç›´æ¥æ¯”è¾ƒ")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°å·²ä¿å­˜çš„PCAç‰¹å¾: {bert_pca_path}")

    # ===== å•å…ƒæ ¼ 9: æµ‹è¯•è¾¹ç•Œæƒ…å†µ =====
    print("\n" + "=" * 80)
    print("[æµ‹è¯•8] è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("=" * 80)

    if X_train_tokens is not None:
        # åˆå§‹åŒ–ä¸€ä¸ªBERT extractorç”¨äºæµ‹è¯•
        try:
            bert_test = CustomBERTExtractor(
                model_name='bert-base-chinese',
                max_length=128,
                batch_size=2,
                use_pretrained=False
            )

            # æµ‹è¯•ç©ºæ–‡æœ¬
            try:
                empty_text = [[]]
                features_empty = bert_test.transform(empty_text)
                print(f"âœ… ç©ºæ–‡æœ¬å¤„ç†: {features_empty.shape}")
            except Exception as e:
                print(f"âš ï¸  ç©ºæ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)[:50]}")

            # æµ‹è¯•é•¿æ–‡æœ¬
            try:
                long_text = [['å­—'] * 200]  # è¶…è¿‡max_length
                features_long = bert_test.transform(long_text)
                print(f"âœ… é•¿æ–‡æœ¬æˆªæ–­: {features_long.shape}")
            except Exception as e:
                print(f"âš ï¸  é•¿æ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)[:50]}")

        except Exception as e:
            print(f"âš ï¸  è¾¹ç•Œæµ‹è¯•è·³è¿‡: {str(e)[:50]}")

    # ===== å•å…ƒæ ¼ 10: æ€»ç»“ =====
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆ!")
    print("=" * 80)

    print("\nâœ… å®Œæˆçš„æµ‹è¯•:")
    print("  1. CustomPCAåŸºæœ¬åŠŸèƒ½æµ‹è¯•")
    print("  2. CustomPCA vs sklearn.PCAå¯¹æ¯”")
    if X_train_tokens is not None:
        print("  3. THUCNewsé¢„å¤„ç†æ•°æ®åŠ è½½")
        if X_train_bert is not None:
            print("  4. BERTç‰¹å¾åŠ è½½/æå–")
            print("  5. CustomPCAé™ç»´BERTç‰¹å¾")
            print("  6. ä¸å·²ä¿å­˜PCAç‰¹å¾å¯¹æ¯”")

    print("\nğŸ“ ç»“è®º:")
    print("  - CustomPCAä½¿ç”¨SVDå®ç°ï¼Œä¸sklearn.PCAç»“æœä¸€è‡´")
    print("  - CustomBERTå®Œæ•´å®ç°Transformeræ¶æ„")
    print("  - å¯åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨è¿™äº›è‡ªå®šä¹‰å®ç°")

    if X_train_tokens is not None and X_train_bert is not None:
        print("\nå¯ç”¨ç‰¹å¾:")
        print(f"  - åŸå§‹BERTç‰¹å¾ (768ç»´): {X_train_bert.shape}")
        print(f"  - PCAé™ç»´ç‰¹å¾ ({n_components}ç»´): {X_train_pca.shape}")

    print("\n" + "=" * 80)
